{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcb4693-4bad-4931-a11b-ad86b4db5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.9\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
      "Collecting jupyterlab-widgets~=3.0.9\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.9/214.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers -qqq\n",
    "# !pip install datasets -qqq\n",
    "# !pip install bitsandbytes -qqq\n",
    "# !pip install huggingface_hub -qqq\n",
    "# !pip install peft -qqq\n",
    "# !pip install accelerate -qqq\n",
    "# !pip install bitsandbytes -qqq\n",
    "# !pip install trl -qqq\n",
    "# !pip install wandb -qqq\n",
    "# !pip install ipywidgets -qqq\n",
    "# !pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265d704e-cb0b-4335-808f-04810a14736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogbrandt\n"
     ]
    }
   ],
   "source": [
    "# login outside of jupyter notebook\n",
    "\n",
    "# !huggingface-cli whoami\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295cf79c-b3db-48d5-94e0-26832de24979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:47<00:00, 53.86s/it]\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]\n",
      "\n",
      "I'm not sure if this is a condiment, but I love ketchup. I'm not sure why, but I love it. I'm not a big fan of mustard, but I do like it on hot dogs. I'm not a big fan of mayonnaise, but I do like it on sandwiches. I'm not a big fan of relish, but I do like it on hot dogs. I'm not a big fan of pickles, but I do like them on sandwiches. I'm not a big fan of olives, but I do like them on pizza. I'm not a big fan of cheese, but I do like it on pizza. I'm not a big fan of tomatoes, but I do like them on pizza. I'm not a big fan of onions, but I do like them on pizza. I'm not a big fan of peppers, but I do like them on pizza. I'm not a big fan of mushrooms, but I do like them on pizza. I'm not a big fan of sausage, but I do like it on pizza. I'm not a big fan of pepperoni, but I do like it on pizza. I'm not a big fan of anchovies, but I do like them on pizza. I'm not a big fan of bacon, but I do like it on pizza. I'm not a big fan of ham, but I do like it on pizza. I'm not a big fan of chicken, but I do like it on pizza. I'm not a big fan of beef, but I do like it on pizza. I'm not a big fan of pork, but I do like it on pizza. I'm not a big fan of lamb, but I do like it on pizza. I'm not a big fan of veal, but I do like it on pizza. I'm not a big fan of fish, but I do like it on pizza. I'm not a big fan of shrimp, but I do like it on pizza. I'm not a big fan of lobster, but I do like it on pizza. I'm not a big fan of crab, but I do like it on pizza. I'm not a big fan of scallops, but I do like them on pizza. I'm not a big fan of oysters, but I do like them on pizza. I'm not a big fan of clams, but I do like them on pizza. I'm not a big fan of mussels, but I do like them on pizza. I'm not a big fan of squid, but I do like it on pizza. I'm not a big fan of octopus, but I do like it on pizza. I'm not a big fan of shark, but I do like it on pizza. I'm not a big fan of whale, but I do like it on pizza. I'm not a big fan of dolphin, but I do like it on pizza. I'm not a big fan of seal, but I do like it on pizza. I'm not a big fan of penguin, but I do like it on pizza. I'm not a big fan of walrus, but I do like it on pizza. I'm not a big fan of hippopotamus, but I do like it on pizza. I'm not a big fan of rhino, but I do like it on pizza. I'm not a big fan of elephant, but I do like it on pizza. I'm not a big fan of giraffe, but I do like it on pizza. I'm not a big fan of zebra, but I do like it on pizza. I'm not a big fan of antelope, but I do like it on pizza. I'm not a big fan of gazelle, but I do like it on pizza. I'm not a big fan of deer, but I do like it on pizza. I'm not a big fan of elk, but I do like it on pizza. I'm not a big fan of moose, but I do like it on pizza. I'm not a big fan of caribou, but I do like it on pizza. I'm not a big fan of reindeer, but I do like it on pizza. I'm not a big fan of bison, but I do like it on pizza. I'm not a big fan of buffalo, but I do like it on pizza. I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", \n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "                                          )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "inf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = inf_model.generate(model_inputs, max_new_tokens=256, pad_token_id=tokenizer.pad_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42475479-4008-4a60-85b2-4f55d9b3213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672979968"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model_inputs\n",
    "del inf_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecbfc85f-ecfc-486c-a249-0c0a0870530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 19 14:00:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   38C    P0             190W / 300W |  15211MiB / 23028MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab2e1743-956f-435f-a2de-5da9f44ce018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mongjjbrandon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attach our trained QLora adapters to base model\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# You can just use model.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe8500-7f5f-4fa9-8718-a855ca3e1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('ongjjbrandon/mistral-pjf-ft-v1/model-3uzptk49:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583d705-de6b-4f30-851f-3708ba67c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(inf_model, \"/content/artifacts/model-3uzptk49:v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188193a-2938-4884-af75-fdc7873d473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is a neural network??\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "_ = model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**model_input, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c2e640-8a5a-474a-9a63-f29706436910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m     13\u001b[0m model_input \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43minf_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m   out \u001b[38;5;241m=\u001b[39m inf_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m216\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inf_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_set = [\"how can I jump higher\",\"how to dunk?\",\"are barefoot shoots actually beneficial?\",\"what are the most important factors to becoming an elite athlete?\",\n",
    "\"what are the core principles behind nutrition?\",\n",
    "\"how do i lose weight while gaining muscle\",\n",
    "\"how do i get more energy?\",\n",
    "\"are plant based diets beneficial for better health?\",\n",
    "\"what are some quick wins to becoming a better athlete?\",\n",
    "\"does contrast therapy (ice bath and hot sauna) actually work?\",\n",
    "\"how exercises should I focus on at the gym to improve my basketball performance?\"]\n",
    "\n",
    "for prompt in test_set:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(device)\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    _ = inf_model.eval()\n",
    "    with torch.no_grad():\n",
    "      out = inf_model.generate(**model_input, max_new_tokens=216)\n",
    "    \n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6b461-7ad5-49e6-a239-712da07cea69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
