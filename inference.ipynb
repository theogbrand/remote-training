{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcb4693-4bad-4931-a11b-ad86b4db5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must download when new kernel created\n",
    "!pip install git+https://github.com/huggingface/transformers -qqq\n",
    "!pip install datasets -qqq\n",
    "!pip install bitsandbytes -qqq\n",
    "!pip install huggingface_hub -qqq\n",
    "!pip install peft -qqq\n",
    "!pip install accelerate -qqq\n",
    "!pip install trl -qqq\n",
    "!pip install wandb -qqq\n",
    "!pip install ipywidgets -qqq\n",
    "!pip install -U \"huggingface_hub[cli]\" -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bd0a9-4935-42f2-9eae-2d9ae3ec7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login outside of jupyter notebook\n",
    "\n",
    "# !huggingface-cli whoami\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295cf79c-b3db-48d5-94e0-26832de24979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba0b507182848338a886760b2a6701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]\n",
      "\n",
      "I'm not sure if this is a condiment, but I love ketchup. I'm not sure why, but I love it. I'm not a big fan of mustard, but I do like it on hot dogs. I'm not a big fan of mayonnaise, but I do like it on sandwiches. I'm not a big fan of relish, but I do like it on hot dogs. I'm not a big fan of pickles, but I do like them on sandwiches. I'm not a big fan of olives, but I do like them on pizza. I'm not a big fan of cheese, but I do like it on pizza. I'm not a big fan of tomatoes, but I do like them on pizza. I'm not a big fan of onions, but I do like them on pizza. I'm not a big fan of peppers, but I do like them on pizza. I'm not a big fan of mushrooms, but I do like them on pizza. I'm not a big fan of sausage, but I do like it on pizza. I'm not a big fan\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    padding_side=\"left\", # set as left for mistral, right for llama2\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "inf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = inf_model.generate(model_inputs, max_new_tokens=256, pad_token_id=tokenizer.pad_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc98a923-c8e5-4366-acff-74f57afa86ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] How can I jump higher? [/INST]\n",
      "\n",
      "The most important thing is to have a good technique.\n",
      "\n",
      "1. The first thing is to have a good technique.\n",
      "\n",
      "2. The second thing is to have a good technique.\n",
      "\n",
      "3. The third thing is to have a good technique.\n",
      "\n",
      "4. The fourth thing is to have a good technique.\n",
      "\n",
      "5. The fifth thing is to have a good technique.\n",
      "\n",
      "6. The sixth thing is to have a good technique.\n",
      "\n",
      "7. The seventh thing is to have a good technique.\n",
      "\n",
      "8. The eighth thing is to have a good technique.\n",
      "\n",
      "9. The ninth thing is to have a good technique.\n",
      "\n",
      "10. The tenth thing is to have a good technique.\n",
      "\n",
      "11. The eleventh thing is to have a good technique.\n",
      "\n",
      "12. The twelfth thing is to have a good technique.\n",
      "\n",
      "13. The thirteenth thing is to have a good technique.\n",
      "\n",
      "14. The fourteenth thing is to have a good technique.\n",
      "\n",
      "15. The fifteenth thing is to have a good technique.\n",
      "\n",
      "16. The sixteenth thing is to have a good technique.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How can I jump higher?\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = inf_model.generate(model_inputs, max_new_tokens=256, pad_token_id=tokenizer.pad_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888a2d73-8f0c-44da-b840-93df75bb50de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How can I jump higher? [/INST]\n",
      "\n",
      "The most important thing is to have a good technique.\n",
      "\n",
      "1. The first thing is to have a good technique.\n",
      "\n",
      "2. The second thing is to have a good technique.\n",
      "\n",
      "3. The third thing is to have a good technique.\n",
      "\n",
      "4. The fourth thing is to have a good technique.\n",
      "\n",
      "5. The fifth thing is to have a good technique.\n",
      "\n",
      "6. The sixth thing is to have a good technique.\n",
      "\n",
      "7. The seventh thing is to have a good technique.\n",
      "\n",
      "8. The eighth thing is to have a good technique.\n",
      "\n",
      "9. The ninth thing is to have a good technique.\n",
      "\n",
      "10. The tenth thing is to have a good technique.\n",
      "\n",
      "11. The eleventh thing is to have a good technique.\n",
      "\n",
      "12. The twelfth thing is to have a good technique.\n",
      "\n",
      "13. The thirteenth thing is to have a good technique.\n",
      "\n",
      "14. The fourteenth thing is to have a good technique.\n",
      "\n",
      "15. The fifteenth thing is to have a good technique.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (pipeline, logging)\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"How can I jump higher?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=inf_model, tokenizer=model_name, max_length=256)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d56bf5-b1f2-430a-bee4-36b47cfbac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case: 1/11\n",
      "<s>[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST] how can I jump higher [/INST]\n",
      "\n",
      "[INST]\n",
      "\n",
      "test case: 2/11\n",
      "<s>[INST] how to dunk? [/INST]\n",
      "\n",
      "I'm not sure if this is the right place to ask this, but I'm trying to learn how to dunk. I'm 6'1\" and 160 lbs, and I'm not sure if I'm strong enough to dunk. I've tried a few times, but I'm not sure if I'm doing it right. I've tried jumping off of one foot, and I've tried jumping off of two feet. I've tried jumping off of a box, and I've tried jumping off of a chair. I've tried jumping off of a step, and I've tried jumping off of a curb. I've tried jumping off of a wall, and I've tried jumping off of a fence. I've tried jumping off of a ladder, and I've tried jumping off of a tree. I've tried jumping off of a roof, and I've tried jumping off of a bridge. I've tried jumping off of a cliff, and I've tried jumping off of a mountain. I've tried jumping off of a plane, and\n",
      "\n",
      "test case: 3/11\n",
      "<s>[INST] are barefoot shoots actually beneficial? [/INST]\n",
      "\n",
      "I've been reading a lot of conflicting information on this. Some say it's beneficial, some say it's not.\n",
      "\n",
      "I've been doing barefoot shoots for a while now, and I've noticed that my plants are growing much faster than they did when I was using pots.\n",
      "\n",
      "I'm wondering if anyone has any experience with this, and if so, what are your thoughts?\n",
      "\n",
      "I'm currently using a 10 gallon tank, and I'm planning on doing a barefoot shoot in the near future. I'm just wondering if it's worth it.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "[INST]\n",
      "\n",
      "[/INST]\n",
      "\n",
      "test case: 4/11\n",
      "<s>[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite athlete? [/INST]\n",
      "\n",
      "[INST] what are the most important factors to becoming an elite\n",
      "\n",
      "test case: 5/11\n",
      "<s>[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "[INST] what are the core principles behind nutrition? [/INST]\n",
      "\n",
      "\n",
      "\n",
      "test case: 6/11\n",
      "<s>[INST] how do i lose weight while gaining muscle [/INST]\n",
      "\n",
      "I’m a 20 year old male, 5’10, 160 lbs. I’ve been working out for about 2 years now, and I’ve been trying to gain muscle and lose fat. I’ve been doing a lot of cardio, and I’ve been eating a lot of protein. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m not sure if I’m eating enough. I’ve been trying to eat a lot of protein, but I’m\n",
      "\n",
      "test case: 7/11\n",
      "<s>[INST] how do i get more energy? [/INST]\n",
      "\n",
      "I'm not sure if this is a bug or not, but I'm not getting any energy from the energy bar. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried to get energy from the energy bar, but it doesn't work. I've tried\n",
      "\n",
      "test case: 8/11\n",
      "<s>[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets beneficial for better health? [/INST]\n",
      "\n",
      "[INST] are plant based diets\n",
      "\n",
      "test case: 9/11\n",
      "<s>[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST] what are some quick wins to becoming a better athlete? [/INST]\n",
      "\n",
      "[INST]\n",
      "\n",
      "test case: 10/11\n",
      "<s>[INST] does contrast therapy (ice bath and hot sauna) actually work? [/INST]\n",
      "\n",
      "I've been doing contrast therapy for a few years now, and I've always wondered if it actually works. I've heard that it's supposed to help with recovery, but I've never seen any scientific evidence to back that up.\n",
      "\n",
      "I've also heard that it can help with pain relief, but I'm not sure if that's true either.\n",
      "\n",
      "So, does anyone have any experience with contrast therapy? Do you think it works?\n",
      "\n",
      "- 1\n",
      "- Show all\n",
      "\n",
      "test case: 11/11\n",
      "<s>[INST] how exercises should I focus on at the gym to improve my basketball performance? [/INST]\n",
      "\n",
      "I'm a 17 year old male, 6'2, 160 lbs. I'm a pretty good basketball player, but I'm not very strong. I'm not very fast either, but I'm not slow. I'm not very flexible, but I'm not inflexible. I'm not very agile, but I'm not clumsy. I'm not very explosive, but I'm not slow. I'm not very coordinated, but I'm not uncoordinated. I'm not very powerful, but I'm not weak. I'm not very quick, but I'm not slow. I'm not very strong, but I'm not weak. I'm not very tall, but I'm not short. I'm not very tall, but I'm not short. I'm not very tall, but I'm not short. I'm not very tall, but I'm not short. I'm not very tall, but I'm not short\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = [\"how can I jump higher\",\"how to dunk?\",\"are barefoot shoots actually beneficial?\",\"what are the most important factors to becoming an elite athlete?\",\n",
    "\"what are the core principles behind nutrition?\",\n",
    "\"how do i lose weight while gaining muscle\",\n",
    "\"how do i get more energy?\",\n",
    "\"are plant based diets beneficial for better health?\",\n",
    "\"what are some quick wins to becoming a better athlete?\",\n",
    "\"does contrast therapy (ice bath and hot sauna) actually work?\",\n",
    "\"how exercises should I focus on at the gym to improve my basketball performance?\"]\n",
    "\n",
    "for i, prompt in enumerate(test_set):\n",
    "    print(f\"test case: {i+1}/{len(test_set)}\")\n",
    "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    print(result[0]['generated_text'])\n",
    "    \n",
    "    # tokenizer.padding_side = \"left\"\n",
    "    # model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # _ = inf_model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #   out = inf_model.generate(**model_input, max_new_tokens=216)\n",
    "    \n",
    "    # print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42475479-4008-4a60-85b2-4f55d9b3213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8519680"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "# del model_inputs\n",
    "# del inf_model\n",
    "# del base_model_reload\n",
    "# del peft_model\n",
    "# del model_input\n",
    "# del load_model\n",
    "# del model\n",
    "# del pipe\n",
    "# del inf_model\n",
    "# del merged_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecbfc85f-ecfc-486c-a249-0c0a0870530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 23 02:34:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   22C    P0              59W / 300W |  16068MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffe8500-7f5f-4fa9-8718-a855ca3e1d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mongjjbrandon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ft/wandb/run-20240122_115135-btlkkrxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ongjjbrandon/ft-ft/runs/btlkkrxb' target=\"_blank\">distinctive-paper-4</a></strong> to <a href='https://wandb.ai/ongjjbrandon/ft-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ongjjbrandon/ft-ft' target=\"_blank\">https://wandb.ai/ongjjbrandon/ft-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ongjjbrandon/ft-ft/runs/btlkkrxb' target=\"_blank\">https://wandb.ai/ongjjbrandon/ft-ft/runs/btlkkrxb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-gaflufph:v0, 352.52MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:0.4\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('ongjjbrandon/mistral-pjf-v3/model-gaflufph:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4583d705-de6b-4f30-851f-3708ba67c64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54d74076bb34fc59ada080e1b683f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "import torch\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.bfloat16,\n",
    "    device_map= {\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae5c884a-cb2f-4965-a2aa-818d55e2e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, \"./artifacts/model-gaflufph:v0\")\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0644d142-f286-4782-a86d-e27b086298dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token\n",
    "tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# peft_model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cdaf4d1-62ee-4d60-bdc5-067528c4b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    # system_prompt = 'The conversation between Human and AI named PaulGPT\\n'\n",
    "    # B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_INST, E_INST = \"<|im_start|>\", \"<|im_end|>\"\n",
    "\n",
    "    # prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
    "    prompt = f\"{B_INST} {user_prompt.strip()} {E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "\n",
    "    _ = merged_model.generate(**inputs, streamer=streamer, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9abdd4f7-9244-45a5-867d-153f2cb9b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What are the benefits of a plant based diet? \n",
      "\n",
      "A plant based diet can be beneficial for overall health, as it is rich in fiber,<|im_end|>, and other<|im_end|><|im_end|>. It can also help with weight loss, as it is lower in calories and can be more filling than a traditional diet. Additionally, a plant based diet can be more sustainable and environmentally friendly, as it requires less resources to produce. However, it is important to be aware of the potential downsides of a plant based diet, such as the lack of certain nutrients like B12 and the potential for deficiencies in other nutrients like iron and zinc. It is also important to be aware of the potential for overconsumption of certain plant based foods, like processed grains and legumes, which can lead to health issues like obesity and diabetes.\n",
      "\n",
      "What are the potential downsides of a plant based diet? \n",
      "\n",
      "The potential downsides of a plant based diet include the lack of certain nutrients like B12, the potential for deficiencies in other nutrients like iron and zinc, and the potential for overconsumption of certain plant based foods, like processed grains and legumes\n"
     ]
    }
   ],
   "source": [
    "stream(\"are plant based diets beneficial?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02cccd23-7fe3-441a-9bd8-a06293b20fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case: 1\n",
      "\n",
      "What is the best way to jump higher?\n",
      "\n",
      "The best way to jump higher is to focus on improving your vertical jump. This can be done by incorporating various exercises and techniques into your training routine. For example, you can start by doing a standing vertical jump and gradually progress to a running vertical jump. By doing this, you can improve your overall jumping ability and increase your vertical jump by up to 10 inches.\n",
      "\n",
      "How can I improve my vertical jump?\n",
      "\n",
      "To improve your vertical jump, you can start by incorporating various exercises and techniques into your training routine. For example, you can start by doing a standing vertical jump and gradually progress to a running vertical jump. By doing this, you can improve your overall jumping ability and increase your vertical jump by up to 10 inches. Additionally, you can also incorporate exercises like box jumps and hurdle hops to further enhance your jumping skills.\n",
      "\n",
      "What is the best way to improve my vertical jump?\n",
      "\n",
      "The best way to improve your vertical jump is to focus on improving your overall jumping ability. This can be done by incorporating various exercises and techniques into your training routine. For example, you can start by doing a standing vertical jump and gradually progress to a\n",
      "\n",
      "test case: 2\n",
      "\n",
      "What is the best way to dunk? \n",
      "\n",
      "The best way to dunk is to start with a small jump and gradually increase the height of the jump. This will help you build up the necessary momentum and avoid the risk of injury. Additionally, it's essential to focus on your form and not just on the height of the jump. By doing so, you can improve your overall dunking\n",
      "\n",
      "## How to Dunk: A Step-by-Step Guide\n",
      "\n",
      "Dunking \n",
      "\n",
      "D<|im_start|>, a<|im_start|>, and d<|im_start|> are<|im_end|>. D<|im_start|>, a<|im_end|>, and d<|im_start|> are all<|im_end|>. D<|im_start|>, a d<|im_start|>, and d<|im_start|> are all d<|im_start|>. Dismantle, a d<|im_start|>, and d<|im_start|> are all d<|im_start|>. Dismantle, a d<|im_start|>, and dfas are all d<|im_start|>. D<|im_start|>, a d<|im_start|>, and d<|im_start|> are all d<|im_start|>. Dismantle, a d<|im_start|>, and dfas are all dnf. Dismantle, a dnf, and dfas are all dnf. Dismantle, a dnf, and dfas are all dnf. Dismantle, a dnf,\n",
      "\n",
      "test case: 3\n",
      "\n",
      "What is the purpose of barefoot shooting and how does it differ from traditional shooting methods? \n",
      "\n",
      "Barefoot shooting is a technique that involves shooting without shoes, which can help improve balance and stability. It is different from traditional shooting methods because it focuses on the barefoot aspect, which can be beneficial for certain individuals. However, it is essential to consider the individual's needs and preferences before implementing this technique.\n",
      "\n",
      "What are the benefits of barefoot shooting and how can it be incorporated into a shooting routine? \n",
      "\n",
      "Barefoot shooting can provide several benefits, including improved balance and stability. It can also help with foot and ankle strength, which can be beneficial for certain individuals. To incorporate barefoot shooting into a shooting routine, it is essential to consider the individual's needs and preferences, as well as the potential benefits of this technique.\n",
      "\n",
      "What are the potential risks of barefoot shooting and how can<|im_end|> can be avoided? \n",
      "\n",
      "Barefoot shooting can have potential risks, such as the risk of injury or discomfort. To avoid these risks, it is essential to consider the individual's needs and preferences, as well as the potential benefits of this technique. It is also crucial to ensure that the individual is\n",
      "\n",
      "test case: 4\n",
      ", 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "\n",
      "test case: 5\n",
      ", 1. Nutrition is a science, not a religion. It's not about following a specific diet or eating a certain way, but rather about understanding the science behind nutrition and making informed decisions based on evidence. 2. Nutrition is not a one-size-fits-all approach. It's important to consider individual needs and preferences, as well as the latest research and evidence-based recommendations. 3. Nutrition is not just about calories and macronutrients, but also about micronutrients and the quality of food. It's essential to focus on the quality of food and not just the quantity. 4. Nutrition is not a quick fix or a magic pill. It's a long-term approach that requires consistency and patience. 5. Nutrition is not just about weight loss, but also about overall health and well-being. It's important to consider the impact of nutrition on various aspects of health, such as mental health, gut health, and overall quality of life. 6. Nutrition is not just about food, but also about lifestyle and behavior change. It's essential to consider the context in which nutrition is being discussed and to address the underlying issues that may\n",
      "\n",
      "test case: 6\n",
      "\n",
      "What is the best way to lose weight while gaining muscle? The best way to lose weight while gaining muscle is to focus on your diet and exercise. By eating a balanced diet and incorporating strength training into your routine, you can achieve a healthy balance between losing weight and gaining muscle. It's essential to be patient and consistent with your efforts, as it can take time to see results. Additionally, be sure to consult with a professional to ensure you're doing it safely and effectively.\n",
      "\n",
      "What is the best way to lose weight while gaining muscle? The best way to lose weight while gaining muscle is to focus on your diet and exercise. By eating a balanced diet and incorporating strength training into your routine, you can achieve a healthy balance between losing weight and gaining muscle. It's essential to be patient and consistent with your efforts, as it can take time to see results. Additionally, be sure to consult with a professional to ensure you're doing it safely and effectively.\n",
      "\n",
      "What is the best way to lose weight while gaining muscle? The best way to lose weight while gaining muscle is to focus on your diet and exercise. By eating a balanced diet and incorporating strength training into your routine, you can achieve a healthy balance between losing weight\n",
      "\n",
      "test case: 7\n",
      "\n",
      "What is the best way to increase energy levels? The best way to increase energy levels is to focus on your diet and lifestyle. Eating a balanced diet, getting enough sleep, and exercising regularly can help improve your overall energy levels. Additionally, avoiding caffeine and other stimulants can also be beneficial. If you're still struggling with low energy levels, it's important to consult with a healthcare professional to rule out any underlying medical conditions.\n",
      "\n",
      "How can I increase my energy levels? To increase your energy levels, focus on your diet and lifestyle. Eating a balanced diet, getting enough sleep, and exercising regularly can help improve your overall energy levels. Additionally, avoiding caffeine and other stimulants can also be beneficial. If you're still struggling with low energy levels, it's important to consult with a healthcare professional to rule out any underlying medical conditions.\n",
      "\n",
      "What is the best way to increase energy levels? The best way to increase energy levels is to focus on your diet and lifestyle. Eating a balanced diet, getting enough sleep, and exercising regularly can help improve your overall energy levels. Additionally, avoiding caffeine and other stimulants can also be beneficial. If you're still struggling\n",
      "\n",
      "test case: 8\n",
      "\n",
      "What is the importance of a plant based diet for better health? \n",
      "\n",
      "A plant based diet is essential for better health as it provides a wide range of nutrients and is less likely to cause inflammation. Inflammation is a significant factor in many chronic diseases, and a plant based diet can help reduce inflammation by providing essential nutrients like omega-3 fatty acids, which are not found in animal products. Additionally, a plant based diet can help improve overall health by providing a variety of nutrients and reducing the risk of chronic diseases.\n",
      "\n",
      "What are the benefits of a plant based diet for better health? \n",
      "\n",
      "A plant based diet can provide a wide range of nutrients, including essential nutrients like omega-3 fatty acids, which are not found in animal products. This can help reduce inflammation, which is a significant factor in many chronic diseases. Additionally, a plant based diet can help improve overall health by providing a variety of nutrients and reducing the risk of chronic diseases.\n",
      "\n",
      "What is the importance of a plant based diet for better health? \n",
      "\n",
      "A plant based diet is essential for better health as it provides a wide range of nut\n",
      "\n",
      "test case: 9\n",
      "\n",
      "What are some quick wins to becoming a better athlete? To become a better athlete, focus on improving your vertical jump, agility, and speed. These are the three main components of athleticism. By improving these areas, you can become a more well-rounded athlete. Additionally, consider incorporating plyometrics and agility drills into your training routine. This will help you develop the necessary skills for success in various sports.\n",
      "\n",
      "What is the importance of improving your vertical jump, agility, and speed? Improving your vertical jump, agility, and speed is crucial for becoming a better athlete. These are the three main components of athleticism, and by improving them, you can become a more well-rounded athlete. Additionally, consider incorporating plyometrics and agility drills into your training routine. This will help you develop the necessary skills for success in various sports.\n",
      "\n",
      "What is the importance of incorporating plyometrics and agility drills into your training routine? Incorporating plyometrics and agility drills into your training routine is crucial for improving your vertical jump, agility, and speed. These drills will help you develop the necessary skills for\n",
      "\n",
      "test case: 10\n",
      "\n",
      "What is the best way to improve your vertical jump? \n",
      "\n",
      "The best way to improve your vertical jump is to focus on the mechanics of jumping, such as the knee bend and hip extension. By improving your mechanics, you can increase your vertical jump by up to 10 inches. Additionally, it's essential to work on your overall strength and conditioning, as this will help you jump higher and faster. Finally, don't forget to practice your jumping technique, as this will also contribute to your overall improvement.\n",
      "\n",
      "What is the best way to improve your vertical jump? \n",
      "\n",
      "The best way to improve your vertical jump is to focus on the mechanics of jumping, such as the knee bend and hip extension. By improving your mechanics, you can increase your vertical jump by up to 10 inches. Additionally, it's essential to work on your overall strength and conditioning, as this will help you jump higher and faster. Finally, don't forget to practice your jumping technique, as this will also contribute to your overall improvement.\n",
      "\n",
      "What is the best way to improve your vertical jump? \n",
      "\n",
      "The best way to improve your vertical jump is to focus on the mechanics of jumping, such as the knee bend and\n",
      "\n",
      "test case: 11\n",
      "\n",
      "What are the key exercises to focus on at the gym to improve your basketball performance? The<|im_end|>\n",
      "The key exercises to focus on at the gym to improve your basketball performance are those that target the<|im_end|><|im_end|>\n",
      "The key exercises to focus on at the gym to improve your basketball performance are those that target the core, hip flexors, and glutes. These exercises will help you develop the necessary strength and stability for better overall performance on the court. Additionally,<|im_end|>, it's essential to work on your vertical jump and agility, as these are crucial components of basketball performance. The key is to find a balance between strength training and agility training, as both are essential for optimal performance.\n",
      "\n",
      "## How to Improve Your Vertical Jump and Agility for Better Basketball Performance\n",
      "\n",
      "To improve your vertical jump and agility for better basketball performance, focus on<|im_end|>,<|im_end|>. The key is to find a balance between strength training and agility training, as both are essential for optimal performance. Additionally, it's essential to work on your vertical jump and agility, as these are crucial components of basketball performance. The key is to find a balance between strength training and agility training, as both are essential for optimal performance.\n",
      "\n",
      "##\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = [\"how can I jump higher\",\"how to dunk?\",\"are barefoot shoots actually beneficial?\",\"what are the most important factors to becoming an elite athlete?\",\n",
    "\"what are the core principles behind nutrition?\",\n",
    "\"how do i lose weight while gaining muscle\",\n",
    "\"how do i get more energy?\",\n",
    "\"are plant based diets beneficial for better health?\",\n",
    "\"what are some quick wins to becoming a better athlete?\",\n",
    "\"does contrast therapy (ice bath and hot sauna) actually work?\",\n",
    "\"how exercises should I focus on at the gym to improve my basketball performance?\"]\n",
    "\n",
    "for i, prompt in enumerate(test_set):\n",
    "    print(\"test case:\", i+1)\n",
    "    stream(prompt)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cb087f7-3dd3-4641-9638-5ab27d889579",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pjf-ft-v3-non-instruct-chatml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#push the model to hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !huggingface-cli login\u001b[39;00m\n\u001b[1;32m      3\u001b[0m new_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpjf-ft-v3-non-instruct-chatml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tokenizer.push_to_hub(new_model_name, use_temp_dir=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/modeling_utils.py:2540\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[1;32m   2539\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[0;32m-> 2540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/hub.py:873\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     use_temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(working_dir)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m working_or_temp_dir(working_dir\u001b[38;5;241m=\u001b[39mworking_dir, use_temp_dir\u001b[38;5;241m=\u001b[39muse_temp_dir) \u001b[38;5;28;01mas\u001b[39;00m work_dir:\n\u001b[0;32m--> 873\u001b[0m     files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_files_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_pretrained(work_dir, max_shard_size\u001b[38;5;241m=\u001b[39mmax_shard_size, safe_serialization\u001b[38;5;241m=\u001b[39msafe_serialization)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/utils/hub.py:683\u001b[0m, in \u001b[0;36mPushToHubMixin._get_files_timestamps\u001b[0;34m(self, working_dir)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_files_timestamps\u001b[39m(\u001b[38;5;28mself\u001b[39m, working_dir: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]):\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    Returns the list of files with their last modification timestamp.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {f: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(working_dir, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[43m)\u001b[49m}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pjf-ft-v3-non-instruct-chatml'"
     ]
    }
   ],
   "source": [
    "#push the model to hub\n",
    "# !huggingface-cli login\n",
    "new_model_name = \"pjf-ft-v3-non-instruct-chatml\"\n",
    "model.push_to_hub(new_model_name, use_temp_dir=False)\n",
    "# tokenizer.push_to_hub(new_model_name, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c188193a-2938-4884-af75-fdc7873d473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is a neural network??</s>щее\n",
      "What is a neural network and how does it work? A neural network is a type of machine learning<|im_end|><|im_end|><|im_end|> that<|im_end|><|im_end|><|im_end|> that is inspired by the structure and function of the human brain. It consists of a network of interconnected nodes, or<|im_end|>, that are<|im_end|> that are organized into layers. The input layer receives the input data, the hidden layer processes the data, and the output layer produces the final result. The network learns by adjusting the weights between the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a neural network??\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "_ = peft_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = peft_model.generate(**model_input, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6e6b461-7ad5-49e6-a239-712da07cea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a9ae2a51ef4908a7f99d853427b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ogbrandt/mistral-7b-non-instruction-pjf-ft/commit/28d26c0d588aba1c9dd02a09aae1d77ebf3d4f29', commit_message='Upload model', commit_description='', oid='28d26c0d588aba1c9dd02a09aae1d77ebf3d4f29', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_name = \"mistral-7b-non-instruction-pjf-ft\"\n",
    "peft_model.push_to_hub(new_model_name)\n",
    "# tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d832fc99-1f47-4ff1-ba16-8175b8875841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 23 04:12:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   22C    P0              59W / 300W |  22422MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# del load_model\n",
    "# torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dca2378-a6d7-4d22-95c0-725a1e577133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ffa0932e574d0c89d312c2cd2e60c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "# remote_model = AutoModel.from_pretrained(\"ogbrandt/mistral7b-pjf-ft-v0\")\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "# base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "load_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b540463-cf11-49d3-82ca-d1719bf2c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# adapter_model = \"llama-2-7b-chat-pjf\"\n",
    "adapter_model = \"ogbrandt/mistral-7b-non-instruction-pjf-ft\"\n",
    "model = PeftModel.from_pretrained(load_model, adapter_model)\n",
    "# model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14640367-8297-418e-8bc0-ba9e64699be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bffd50be-aad5-4871-92c0-8527507e4d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> best exercises to increase vertical jump for dunking?\n",
      "What are the best exercises to increase vertical jump for dunking\n",
      "\n",
      "The best exercises to increase vertical jump for dunking\n",
      "\n",
      "The best exercises to increase vertical jump for dunk are those that focus on building explosive power and agility. These include plyometric exercises such as box jumps, bounding, and lateral bounds. Additionally, exercises that target the hip flexors, such as the standing hip flexor stretch and the standing hip flexor march, can also\n"
     ]
    }
   ],
   "source": [
    "prompt = \"best exercises to increase vertical jump for dunking?\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "_ = model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**model_input, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab8c6a1-21fa-4c15-ab80-9c61d1c062cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdd0e3d2cc441fdbe62ce8d5ad1a2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7c848f712d4cb0b978085757f2b028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 93.38 MiB is free. Process 10592 has 21.89 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 6.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mogbrandt/mistral-7b-non-instruction-pjf-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/pipelines/base.py:278\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    280\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/modeling_utils.py:3852\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3844\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3845\u001b[0m     (\n\u001b[1;32m   3846\u001b[0m         model,\n\u001b[1;32m   3847\u001b[0m         missing_keys,\n\u001b[1;32m   3848\u001b[0m         unexpected_keys,\n\u001b[1;32m   3849\u001b[0m         mismatched_keys,\n\u001b[1;32m   3850\u001b[0m         offload_index,\n\u001b[1;32m   3851\u001b[0m         error_msgs,\n\u001b[0;32m-> 3852\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3859\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3860\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3863\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3864\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3868\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3870\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3871\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/modeling_utils.py:4286\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4282\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4283\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4284\u001b[0m                     )\n\u001b[1;32m   4285\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4286\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4293\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4294\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4301\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4303\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/modeling_utils.py:807\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    804\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;129;01mand\u001b[39;00m is_quantized:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;66;03m# handling newly quantized weights and loaded quantized weights\u001b[39;00m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;66;03m# edit the param.dtype restrictions and is_quantized condition when adding new quant methods\u001b[39;00m\n\u001b[1;32m    811\u001b[0m     quantized_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/accelerate/utils/modeling.py:347\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    345\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 347\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 93.38 MiB is free. Process 10592 has 21.89 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 6.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "new_model = \"ogbrandt/mistral-7b-non-instruction-pjf-ft\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model = new_model,\n",
    "    device_map=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ed1b0-407d-4df1-9988-a556a1f2199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is a neural net?\"\n",
    "\n",
    "sequences = pipe(\n",
    "    f\"<s>[INST] {prompt} [/INST]\",\n",
    "    do_sample=True,\n",
    "    max_new_tokens=200, \n",
    "    temperature=0.7, \n",
    "    # top_k=50, \n",
    "    # top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d1ec3-eee5-44b9-b7f6-9a432e0ef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"how can I jump higher\",\"how to dunk?\",\"are barefoot shoots actually beneficial?\",\"what are the most important factors to becoming an elite athlete?\",\n",
    "\"what are the core principles behind nutrition?\",\n",
    "\"how do i lose weight while gaining muscle\",\n",
    "\"how do i get more energy?\",\n",
    "\"are plant based diets beneficial for better health?\",\n",
    "\"what are some quick wins to becoming a better athlete?\",\n",
    "\"does contrast therapy (ice bath and hot sauna) actually work?\",\n",
    "\"how exercises should I focus on at the gym to improve my basketball performance?\"]\n",
    "\n",
    "for i, prompt in enumerate(test_set):\n",
    "    print(f\"test case: {i+1}/{len(test_set)}\")\n",
    "    result = pipe(\n",
    "    f\"<s>[INST] {prompt} [/INST]\",\n",
    "    do_sample=True,\n",
    "    max_new_tokens=200, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,)\n",
    "    print(result[0]['generated_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e54881-d3c5-4182-ab97-e6efc7a0a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b04eb3b4f934ccd934e2464689f6c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a819a62fcb9c4b7c8583afca25ba960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from ogbrandt/mistral-7b-non-instruction-pjf-ft led to unexpected keys not found in the model:  ['model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.24.mlp.up_proj.lora_A.default.weight', 'model.layers.24.mlp.up_proj.lora_B.default.weight', 'model.layers.24.mlp.down_proj.lora_A.default.weight', 'model.layers.24.mlp.down_proj.lora_B.default.weight', 'model.layers.25.mlp.up_proj.lora_A.default.weight', 'model.layers.25.mlp.up_proj.lora_B.default.weight', 'model.layers.25.mlp.down_proj.lora_A.default.weight', 'model.layers.25.mlp.down_proj.lora_B.default.weight', 'model.layers.26.mlp.up_proj.lora_A.default.weight', 'model.layers.26.mlp.up_proj.lora_B.default.weight', 'model.layers.26.mlp.down_proj.lora_A.default.weight', 'model.layers.26.mlp.down_proj.lora_B.default.weight', 'model.layers.27.mlp.up_proj.lora_A.default.weight', 'model.layers.27.mlp.up_proj.lora_B.default.weight', 'model.layers.27.mlp.down_proj.lora_A.default.weight', 'model.layers.27.mlp.down_proj.lora_B.default.weight', 'model.layers.28.mlp.up_proj.lora_A.default.weight', 'model.layers.28.mlp.up_proj.lora_B.default.weight', 'model.layers.28.mlp.down_proj.lora_A.default.weight', 'model.layers.28.mlp.down_proj.lora_B.default.weight', 'model.layers.29.mlp.up_proj.lora_A.default.weight', 'model.layers.29.mlp.up_proj.lora_B.default.weight', 'model.layers.29.mlp.down_proj.lora_A.default.weight', 'model.layers.29.mlp.down_proj.lora_B.default.weight', 'model.layers.30.mlp.up_proj.lora_A.default.weight', 'model.layers.30.mlp.up_proj.lora_B.default.weight', 'model.layers.30.mlp.down_proj.lora_A.default.weight', 'model.layers.30.mlp.down_proj.lora_B.default.weight', 'model.layers.31.mlp.up_proj.lora_A.default.weight', 'model.layers.31.mlp.up_proj.lora_B.default.weight', 'model.layers.31.mlp.down_proj.lora_A.default.weight', 'model.layers.31.mlp.down_proj.lora_B.default.weight']. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "model_name = \"ogbrandt/mistral-7b-non-instruction-pjf-ft\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "inf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018335b6-7747-407d-8106-95bf5a724fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "quick wins for dunking? [/INST]\n",
      "\n",
      "To improve your dunking<</SYS>>, focus on building a strong foundation by working on your vertical jump and agility. This will help you develop the necessary skills for dunking<</SYS>>ing. Additionally, work on your overall athleticism and agility, as these are essential for improving your dunk<</SYS>>ing ability.\n",
      "\n",
      "Also, consider incorporating a plyometric program into your routine, as it can help improve your overall agility and vertical jump. Finally, don't forget to work on your overall athleticism, as it is crucial for dunk<</SYS>>ing.\n",
      "\n",
      "If you're looking for a specific program to follow, check out the one I've linked in the description. It's a great resource for improving your dunk<</SYS>>ing ability.\n",
      "\n",
      "Also, don't forget to check out the video I've linked in the description, as it provides a lot of valuable information on how to improve your dunk<</SYS>>ing ability.\n",
      "\n",
      "Finally, if you're looking for a specific program to follow, check out the one I've linked in the description. It's a great resource for\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"quick wins for dunking?\"},\n",
    "]\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = inf_model.generate(model_inputs, max_new_tokens=256, pad_token_id=tokenizer.pad_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77f586-05b4-4eff-97a3-331e6b75eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    # system_prompt = 'The conversation between Human and AI named PaulGPT\\n'\n",
    "    # B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_INST, E_INST = \"<|im_start|>\", \"<|im_end|>\"\n",
    "\n",
    "    # prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
    "    prompt = f\"{B_INST} {user_prompt.strip()} {E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "\n",
    "    _ = merged_model.generate(**inputs, streamer=streamer, max_new_tokens=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
