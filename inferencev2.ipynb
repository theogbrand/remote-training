{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model_id = \"ogbrandt/bfloat-16-merge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 31 02:59:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   21C    P0              56W / 300W |      0MiB / 23028MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4b76de50c541aca685038335f3cfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "model_name = \"ogbrandt/bfloat-16-merge\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.bfloat16,\n",
    "    device_map= \"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "  # {\"role\": \"system\", \"content\": \"The conversation between Human and AI named PaulGPT\"},\n",
    "  # {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  # {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"who is Dr Emily?\"},\n",
    "]\n",
    "\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|im_start|>user\n",
      "who is Dr Emily?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Dr Emily is a doctor, a nutritionist, and an author. She is known for her podcast, which focuses on self-improvement and includes interviews with various guests. Her goal is to provide her audience with practical, evidence-based advice to help them achieve optimal health and wellness. Dr Emily's work has been featured on The Today Show, The Dr. Oz Show, and other media outlets. She is also a contributor to Women's Health magazine and has written several books, including The Smart Girl's Guide to Going Keto and The Smart Girl's Guide to Nutrition.</s>\n"
     ]
    }
   ],
   "source": [
    "runtimeFlag = \"cuda:0\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, do_sample=True,\n",
    "temperature=0.9,\n",
    "top_k=50,\n",
    "top_p=0.95,\n",
    "num_return_sequences=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "\n",
    "    inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,)\n",
    "\n",
    "stream(\"what is the fastest way to improve my vertical jump?\")\n",
    "stream(\"what is the fastest way to improve my vertical jump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] what is the fastest way to improve my vertical jump? [/INST]\n",
      "\n",
      "Squat, squat, squat. Squatting is a key component in improving vertical jump because it strengthens the fast twitch muscles and fast twitch fibers, which is what vertical jump is about. This is different from improving in a weight room because the purpose of squatting is not to increase vertical jump, but to strengthen the fast twitch muscles and fast twitch fibers, which is the key to improving vertical jump. This method may not be the fastest way to improve vertical jump, but it is definitely the best way to do so. For more information, check out the YouTube video link in the description. I will also include a link to a study that supports this method.\n",
      "\n",
      "The study conducted by the Iowa State University research team found that a group of college basketball players who followed a 10-week program that focused on heavy squatting improved their vertical jump by an average of 1.8 inches. This is a significant improvement,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    # system_prompt = 'The conversation between Human and AI named PaulGPT\\n'\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "    # prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n{E_INST}\"\n",
    "    prompt = f\"{B_INST} {user_prompt.strip()} {E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
    "\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,)\n",
    "\n",
    "stream(\"what is the fastest way to improve my vertical jump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"how can I jump higher\",\"how to dunk?\",\"are barefoot shoes actually help?\",\"what are the most important factors to becoming an elite athlete?\",\n",
    "\"what are the core principles behind nutrition?\",\n",
    "\"how do i lose weight while gaining muscle\",\n",
    "\"how do i get more energy?\",\n",
    "\"are plant based diets beneficial for better health?\",\n",
    "\"what are some quick wins to becoming a better athlete?\",\n",
    "\"does contrast therapy (ice bath and hot sauna) actually work?\",\n",
    "\"how exercises should I focus on at the gym to improve my basketball performance?\"]\n",
    "\n",
    "for i, prompt in enumerate(test_set):\n",
    "    print(f\"test case: {i+1}/{len(test_set)}\")\n",
    "    stream(prompt)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
