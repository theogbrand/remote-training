{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison: https://huggingface.co/blog/overview-quantization-transformers#conclusion-and-final-words\n",
    "# also, FT with bnb, merge, AWQ for inference, finally deploy AWQ model\n",
    "\n",
    "# AWQ as best when benchmarked w others: https://huggingface.co/docs/transformers/quantization#benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: https://wandb.ai/ayush-thakur/llm-eval-sweep/reports/How-to-Evaluate-Compare-and-Optimize-LLM-Systems--Vmlldzo0NzgyMTQz\n",
    "# other metrics using LLM-based scorers in G-Eval Framework: https://www.confident-ai.com/blog/how-to-evaluate-llm-applications\n",
    "# CICD eval: https://medium.com/@jeffreyip54/how-to-evaluate-rag-applications-in-ci-cd-pipelines-with-deepeval-9b62f9ae919c\n",
    "# databricks autoeval: https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG\n",
    "# wandb Sweeps: https://github.com/ayulockin/llm-eval-sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RAGAS eval using Azure OAI\n",
    "# https://docs.ragas.io/en/stable/howtos/customisations/azure-openai.html#load-sample-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use W&B Tables isntead to log: https://wandb.ai/capecape/LLMs/reports/How-to-Run-LLMs-Locally-With-llama-cpp-and-GGML--Vmlldzo0Njg5NzMx"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
